<html>
    <head>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-126051187-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-126051187-1');
        </script>

        <title>a8b.io</title>
        <link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="../../index.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.11.1/tocbot.css">
        <link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
    </head>
    <body>
        <div class="main-wrapper">
            <div class="header-wrapper">
                <h1 class="title"><a href="../index.html">a8b.io</a></h1>
            </div>
            <div class="content-wrapper">
                <div class="content-header-wrapper">
                    <span class="up-nav">
                        <a href="../index.html">All Posts</a>
                    </span>
                    <span class="social-item">Follow me on twitter <a href="https://twitter.com/AmeerNAyoub"><img src="../../images/mini_me.jpg" class="mini-profile-image" />@AmeerNAyoub</a></span>
                </div>
                <div class="post column-container" id="reddit-10k">
                    <div class="left">
                        <div class="post-title">
                            <h1>How I got 10,000 post karma on reddit with (and without) fast.ai</h1>
                            <span class="post-date">October 29, 2020</span>
                        </div>
                        <div class="post-content">
                            <div class="figure">
                                <img src="./images/reddit_badge.png" />
                                <span class="caption">Profile image made by <a href="https://ai-art.tokyo/en/">AI Gahaku</a></span>
                            </div>

                            <h2 id="motivation">Motivation</h2>
                            <p>Back in the late 2006 two of my best friends and I just starting college took ENG 105 at Arizona State University with Don Ownsby. Most notably of that class the text book was called "Everything's an Argument". Also in this class was Suhail Doshi (@Suhail), about whom I remember nothing other than his presence, a testament to the role of context in achievment and how uninteresting this class was to me. A year or so later one of these hypercompetetive friends and I put together a spread sheet of ~20 high level achievments called "Everything's a Contest". This includes things like "Watch the IMDB top 250 movies" (which has notable changed since then), "Photograph a live grizzly bear", and "Have something named after you" (which couldn't be done by anyone we know personally). In this list was also "Get 10,000 (post) karma on reddit". We were very into reddit (and digg) back then, something that has since fallen out of both of our lives. Real life got in the way of Everything's a Contest and we didn't really care about the results and so we found ourselves a dozen or so years later with no one having finished any of them. In early 2020 I decided I should finish more of what I start and so I decided to tackle one of these long standing contests, but I wanted to kill two birds with one stone. I was going to do it with AI (or so I thought) since I wanted to do more real life applications of deep learning. I'm a huge proponent of <a href="http://fast.ai/">fast.ai</a> and I love the way you can tackle real problems quickly with its high level abstractions. For someone trying to get into AI I would highly recommend it as a framework and the associated material. This is a story about how I used AI  and other strategies to gain ~10,000 post karma on reddit.
                            </p>

                            <h2 id="the-approach">The Approach</h2>
                            <p>The two classes of content on reddit in general, generative and aggregative. If tackling the problem from a generative approach, then you're looking at creating original content (like on r/memes) and posting that content. The <A href="https://imgflip.com/ai-meme">imgflip AI meme generator</a> for example generates content that could be posted to r/memes. While the memes that are generated are amusing they generally aren't as good as memes that are hand written. At some point I did post one of those.</p>

                            <div class="figure">
                                <img src="./images/3migr03n6m251.jpg" height="200" />
                                <span class="caption">This post got ~35 points with the title "An AI generated this meme. Good to know it's putting it's sentience to good use."</span>
                            </div>

                            <p>The other approach would be finding content online and reposting it which I'm calling the aggregative approach. I chose this route because generating high quality articles and images is by far more involved. With the aggregative approach, rather than being disadvantaged in that you are trying to catch up to human quality you are at the advantage since the primary differentiator is speed (who finds and posts this thing first). In particular I looked at <i>news</i> based subreddits, as there are little chances of things being a repost and lots of quality candidates being generated frequently. News based subreddits tend to have large member bases, and a lot of the content on the news subreddits comes from the same set of known sites which are easy to watch and crawl.</p>

                            <div class="figure">
                                <iframe class="airtable-embed" src="https://airtable.com/embed/shr9FUw82wAZBoQwn?backgroundColor=cyan&viewControls=on" frameborder="0" onmousewheel="" width="100%" height="533" style="background: transparent; border: 1px solid #ccc;"></iframe>
                                <span class="caption">All the > 1 score articles from /r/business in the last week (recorded on 2020/10/29), illustrates how a few sites supply the majority of the content for news based subreddits.</span>
                            </div>

                            <p>Before I go any further I want to lay out a principle here with the approach I took. At this point we could go the spam route, and forego the need for AI at all. Simply crawl certain news websites and spam articles to relevant subreddits. However this seems like an easy route to getting banned from a subreddit. For example /r/worldnews (25.3m members), one of the subreddits I targeted disallows US news. Submitting articles related to the US results in those posts being removed. /r/business specifically disallows spamming. While in theory I might have gotten away with it or used some sort of generic rate limiting and hope for the best, I wanted my bot to be a productive member of the community, submitting thoughtful content, rather than a spam bot that would be banned in a few hours.</p>

                            <p>I targeted /r/business and /r/worldnews, primarily because those areas interested me. /r/business was relatively small (577k members) and I could risk getting banned there so I went for that first. I wanted to train a bot to crawl these top sites, determine if a page was a quality article and submit the post if it was.</p>

                            <h2>The Setup</h2>
                            <h3 id="posts">Finding and loading posts</h3>
                            <p>I need to train a NLP model to evaluate article text and produce some sort of class output. I chose to model the output into "bins" with different cutoffs rather than a continuous output like in a regression model which I'll show later. In order to train the model I needed articles that were submitted to reddit and their corresponding scores. Unfortunately the official reddit API limits the amount of historic post data you can retrieve to 1000 items. Therefore I grabbed historic posts from <a href="https://pushshift.io/">pushshift.io</a> using code I shamelessly adopted from <a href="https://www.osrsbox.com/blog/2019/03/18/watercooler-scraping-an-entire-subreddit-2007scape/">WaterCooler: Scraping an Entire Subreddit (/r/2007scape)</a>. The output file of the script had individual lines like the below, one line per post:</p>

<pre style="white-space: pre-wrap;">{"author": "CALIPHATEMEDIA", "author_flair_css_class": null, "author_flair_text": null, "brand_safe": true, "can_mod_post": false, "contest_mode": false, "created_utc": 1514767344, "domain": "caliphatemedia.info", "full_link": "https://www.reddit.com/r/business/comments/7nc5yf/south_korea_to_regulate_bitcoin_trading_further/", "id": "7nc5yf", "is_crosspostable": false, "is_reddit_media_domain": false, "is_self": false, "is_video": false, "locked": false, "num_comments": 0, "num_crossposts": 0, "over_18": false, "parent_whitelist_status": "all_ads", "permalink": "/r/business/comments/7nc5yf/south_korea_to_regulate_bitcoin_trading_further/", "pinned": false, "retrieved_on": 1514841750, "score": 1, "selftext": "", "spoiler": false, "stickied": false, "subreddit": "business", "subreddit_id": "t5_2qgzg", "subreddit_type": "public", "thumbnail": "default", "thumbnail_height": 140, "thumbnail_width": 140, "title": "South korea to regulate bitcoin trading further with tougher measures", "url": "http://www.caliphatemedia.info/2017/12/south-korea-govt-to-introduce-tougher.html", "whitelist_status": "all_ads"}</pre>

                            <p>I loaded the url into <a href="https://github.com/slaveofcode/boilerpipe3">boilerpipe3</a> which transforms full HTML articles into simple text.</p>

                            <div class="figure">
                                <img src="./images/boilerpipe_example.png" />
                                <span class="caption">An example of boilerpipe3 on the reuters article <a href="https://www.reuters.com/article/us-amazon-com-results/amazon-forecasts-jump-in-holiday-sales-and-pandemic-costs-idUSKBN27E3DV">"Amazon forecasts jump in holiday sales - and pandemic costs"</a></span>
                            </div>

                            <p>After loading the article text I did some processing to remove items which didn't load properly and truncate long articles to ~10k characters.</p>

                            <h3 id="training">Training the model</h3>
                            I trained the model on 271k articles, from 2018-01-01 to mid april 2020. I used a <a href="https://docs.fast.ai/text.models.awdlstm">AWD_LSTM</a> for the language model and for text classification using buckets of neutral (0-10 karma), okay (10-100), good (100-500), and great (500+). Note that the bins are kind of arbitrary and I've changed them around for different models and subreddits. The only thing I actually ended up looking at runtime was the non-neutral score, so this could have been a binary classifier.

                            <pre>TODO insert a gist of the notebook you used</pre>

                            <h3 id="personal-aggregator">My personal business news aggregator</h3>
                            <p>At this point I chose a few sites (business insider, reuters, bloomberg, cnn, cnbc, bbc, etc.) based on the top sites for /r/business and built a crawler using requests and BeautifulSoup. Every minute or so I'd crawl the site root for new links and process the linked page with boilerpipe and pass it through the model. Items with a <i>non-neutral</i> score of > 0.25 would be flagged and emailed to me using AWS SNS. Initially I relied on filtering these incoming suggestions and submitting the articles myself using the reddit app.</p>

                            <div class="figure">
                                <img src="./images/example_email.png" />
                                <span class="caption">An example email I received from my site poller.</span>
                            </div>

                            <h2 id="automation">Automation</h2>
                            <p>The thing to tackle here was filtering these suggestions, and gathering an appropriate title for the reddit post. The page title is usually not a good title, and the title should instead be extracted from the content for example by picking the most appropriate h1 tag on the page. Rather than refining the model to be better at classifying articles, and training or coding a new component for extracting the title I decided to try to encapsulate these tasks into a mechanical turk task and have humans do the final gate keeping and title selection for me. I can take the results of those mturk tasks and submit it to reddit via the reddit API. I built this all into my crawler.</p>

                            <h3 id="mturk">Mechanical Turk</h3>
                            <p></p>

                            <h2 id="results">Results</h2>
                        </div>
                        <div class="post-tags">
                            <ul>
                                <li>#reddit #everythings-a-contest</li>
                            </ul>
                        </div>
                    </div>
                    <div class="right">
                        <div class="post-toc">
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- TOC Bot -->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.11.1/tocbot.min.js"></script>
        <script type="text/javascript">
            for (post of document.getElementsByClassName('post')) {
                const identifier = post.getAttribute("id");

                tocbot.init({
                    tocSelector: `#${identifier} .post-toc`,
                    contentSelector: `#${identifier} .post-content`,
                    headingSelector: 'h1, h2, h3',
                    hasInnerContainers: true,
                });
            }
        </script>
    </body>
</html>
