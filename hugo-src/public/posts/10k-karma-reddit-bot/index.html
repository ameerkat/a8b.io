<!doctype html>

<html lang="en-us">

<head>
  <title>How I got 10k post karma on reddit with (and without) fast.ai - a8b.io</title>
  <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta name="description" content="The HTML5 Herald" />
<meta name="author" content="" /><meta property="og:title" content="How I got 10k post karma on reddit with (and without) fast.ai" />
<meta property="og:description" content="Profile image made by AI GahakuMotivation Back in late 2006 two of my best friends and I just starting college took ENG105 at Arizona State University. The most memorable thing from that class was the textbook called &ldquo;Everything&rsquo;s an Argument&rdquo;, the title of which we used as justification to argue about mundane topics in jest. A year or so later one of these friends and I put together a spreadsheet of 20 or so high-level achievements called Everything&rsquo;s a Contest." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.a8b.io/posts/10k-karma-reddit-bot/" />
<meta property="article:published_time" content="2020-10-31T15:10:56-07:00" />
<meta property="article:modified_time" content="2020-10-31T15:10:56-07:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="How I got 10k post karma on reddit with (and without) fast.ai"/>
<meta name="twitter:description" content="Profile image made by AI GahakuMotivation Back in late 2006 two of my best friends and I just starting college took ENG105 at Arizona State University. The most memorable thing from that class was the textbook called &ldquo;Everything&rsquo;s an Argument&rdquo;, the title of which we used as justification to argue about mundane topics in jest. A year or so later one of these friends and I put together a spreadsheet of 20 or so high-level achievements called Everything&rsquo;s a Contest."/>

<meta name="generator" content="Hugo 0.77.0" />
    

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />
  <link rel="stylesheet" href="https://www.a8b.io/fontawesome/css/all.min.css" />
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Slab|Ruda" />
  
  
  <link rel="stylesheet" type="text/css" href="/css/styles.css" /></head>

<body>
  <div id="container">
    <header>
      <h1>
                <a href="/">a8b.io</a>
            </h1>

      <ul id="social-media">
             <li>
               <a href="https://twitter.com/AmeerNAyoub" title="Twitter">
               <i class="fab fa-twitter fa-lg"></i>
               </a>
             </li>
      </ul>
      
    </header>

    
<nav>
    <ul>
        
    </ul>
</nav>


    <main>




<article>

    <h1>How I got 10k post karma on reddit with (and without) fast.ai</h1>

    
      <aside>
    <ul>
        <li>
            <time class="post-date" datetime="2020-10-31T15:10:56-07:00">Oct 31, 2020</time>
        </li>
        

        
        <li>
            <em>
                
                    
                    <a href="/tags/reddit">#reddit</a>
                
                    , 
                    <a href="/tags/fast-ai">#fast-ai</a>
                
                    , 
                    <a href="/tags/nlp">#nlp</a>
                
            </em>
        </li>
        

        <li>12 minutes read</li>
    </ul>
</aside>

    

    
<div class="featured_image">
    <a href="https://www.a8b.io/posts/10k-karma-reddit-bot/" title="How I got 10k post karma on reddit with (and without) fast.ai">
        <img src="">
    </a>
</div>



    <p>
  <figure>
    <img src="/images/reddit/reddit_badge.png#mid" alt="My reddit profile with 10,088 post karma">
    <figcaption>Profile image made by AI Gahaku</figcaption>
  </figure>
</p>
<h2 id="motivation">Motivation</h2>
<p>Back in late 2006 two of my best friends and I just starting college took ENG105 at Arizona State University. The most memorable thing from that class was the textbook called &ldquo;Everything&rsquo;s an Argument&rdquo;, the title of which we used as justification to argue about mundane topics in jest. A year or so later one of these friends and I put together a spreadsheet of 20 or so high-level achievements called Everything&rsquo;s a Contest. This includes items like &ldquo;Watch the IMDB top 250 movies&rdquo; (which has notably changed since then), &ldquo;Photograph a live grizzly bear in the wild&rdquo;, and &ldquo;Have something named after you&rdquo; (which couldn&rsquo;t be done by anyone you know personally). On this list was also &ldquo;Get 10,000 (post) karma on Reddit&rdquo;. Despite our heated discussions about what should be on this list and the criteria for success none of us ever really did anything substantial to complete any of these goals. In early 2020 I decided I should finish more of what I start, and so I decided to tackle one of these long-standing contests. But I wanted to kill two birds with one stone. I was going to do it with AI, since I wanted to apply AI to more real-life problems. I&rsquo;m a huge proponent of <a href="https://www.fast.ai/">fast.ai</a> and I love the way you can tackle real problems quickly with its high-level abstractions. For someone trying to get into AI I would highly recommend using it as a framework and going through the associated materials. This is a post about how I used AI and other strategies to gain ~10,000 post karma on Reddit.</p>
<h2 id="approach">Approach</h2>
<p>Content on Reddit, in general, falls into two categories: generative (original content) and aggregative (posting found content). Trying to automate generating content to post would be implementing something like the <a href="https://imgflip.com/ai-meme">imgflip AI meme generator</a> and posting resulting content to r/memes. While the memes that are generated are an amusing juxtaposition of tropes, they generally aren&rsquo;t as good as memes that are generated by human users skilled in the art of observational comedy. Try the meme generator out and you&rsquo;ll see what I mean. At some point, I did try posting one of those just to see how well it did.</p>
<p>
  <figure>
    <img src="/images/reddit/3migr03n6m251.jpg#mid" alt="">
    <figcaption>I went through about 100 auto generated memes before I got this. This post got ~35 points with the title &#39;An AI generated this meme. Good to know it&#39;s putting its sentience to good use.&#39;</figcaption>
  </figure>
</p>
<p>The other approach would be finding content online and reposting it which I&rsquo;m calling the aggregative approach. I chose this route because while generating high-quality content is more interesting it&rsquo;s also far more challenging and involved. With the aggregative approach, rather than being disadvantaged in that you are trying to catch up to the &ldquo;human quality&rdquo; of content a computer is at the advantage since the primary differentiator between posters is the ability to search through large amounts of content and speed (who finds and posts something first). In particular, I ended up looking at new based subreddits:</p>
<ul>
<li>There are little chances of things being a repost if you post fast enough</li>
<li>There are lots of quality candidates to post being generated frequently all over the internet</li>
<li>Tend to have large member bases</li>
<li>A lot of the content on the news subreddits comes from the same set of known sites that are easy to enumerate for watching and crawling</li>
</ul>
<div class="figure">
    <iframe class="airtable-embed" src="https://airtable.com/embed/shr9FUw82wAZBoQwn?backgroundColor=cyan&viewControls=on" frameborder="0" onmousewheel="" width="100%" height="533" style="background: transparent; border: 1px solid #ccc;"></iframe>
    <span class="caption">Domains for all the > 1 score articles from /r/business in the last week (recorded on 2020/10/29) with their counts of occurence. This illustrates how a few sites supply the majority of the content for news based subreddits with over half the articles coming from the top 5 sites.</span>
</div>
<p>I should note that I have to use my primary account for this (not using a novelty account was a requirement to accomplishing this task). At this point, I could go the spam route, and forego the need for AI at all by simply crawling certain news websites and spamming articles to relevant subreddits. In general spamming on reddit is looked down upon, thought I believe it&rsquo;s up to the discretion of the subreddit moderators as far as I know. While in theory I might have gotten away with it or used some sort of generic rate-limiting and hope for the best, I wanted my bot to be more like a productive member of the community submitting thoughtful content, rather than a spam bot that would be the bane of its existence until forcibly removed since I needed it to run long-term on the same account.</p>
<p>Specifically for news subreddits I targeted /r/business and /r/worldnews, somewhat arbitrarily and somewhat because those areas seemed interesting to me. /r/business was relatively small (577k members) and low frequency for a news subreddit so it seemed approachable as a first target. The goal was to build a web crawler to watch popular business news sites (like the ones I listed in the table above) that would leverage an NLP-based article classifier to determine if the article was worthy of being posted (had a high chance of receiving upvotes).</p>
<h2 id="setup">Setup</h2>
<h3 id="finding-and-loading-posts">Finding and loading posts</h3>
<p>To train the NLP model that will classify articles I need the article text for the article submitted to Reddit and their corresponding Reddit scores. Unfortunately, the official Reddit API limits the amount of historic post data you can retrieve to 1000 items. Therefore I grabbed historic posts from <a href="https://pushshift.io/">pushshift.io</a> using code I shamelessly adapted from <a href="https://www.osrsbox.com/blog/2019/03/18/watercooler-scraping-an-entire-subreddit-2007scape/">WaterCooler: Scraping an Entire Subreddit (/r/2007scape)</a>. The output file of the script has individual lines like the one below, containing a JSON object of a post per line:</p>
<pre><code>{&quot;author&quot;: &quot;CALIPHATEMEDIA&quot;, &quot;author_flair_css_class&quot;: null, &quot;author_flair_text&quot;: null, &quot;brand_safe&quot;: true, &quot;can_mod_post&quot;: false, &quot;contest_mode&quot;: false, &quot;created_utc&quot;: 1514767344, &quot;domain&quot;: &quot;caliphatemedia.info&quot;, &quot;full_link&quot;: &quot;https://www.reddit.com/r/business/comments/7nc5yf/south_korea_to_regulate_bitcoin_trading_further/&quot;, &quot;id&quot;: &quot;7nc5yf&quot;, &quot;is_crosspostable&quot;: false, &quot;is_reddit_media_domain&quot;: false, &quot;is_self&quot;: false, &quot;is_video&quot;: false, &quot;locked&quot;: false, &quot;num_comments&quot;: 0, &quot;num_crossposts&quot;: 0, &quot;over_18&quot;: false, &quot;parent_whitelist_status&quot;: &quot;all_ads&quot;, &quot;permalink&quot;: &quot;/r/business/comments/7nc5yf/south_korea_to_regulate_bitcoin_trading_further/&quot;, &quot;pinned&quot;: false, &quot;retrieved_on&quot;: 1514841750, &quot;score&quot;: 1, &quot;selftext&quot;: &quot;&quot;, &quot;spoiler&quot;: false, &quot;stickied&quot;: false, &quot;subreddit&quot;: &quot;business&quot;, &quot;subreddit_id&quot;: &quot;t5_2qgzg&quot;, &quot;subreddit_type&quot;: &quot;public&quot;, &quot;thumbnail&quot;: &quot;default&quot;, &quot;thumbnail_height&quot;: 140, &quot;thumbnail_width&quot;: 140, &quot;title&quot;: &quot;South korea to regulate bitcoin trading further with tougher measures&quot;, &quot;url&quot;: &quot;http://www.caliphatemedia.info/2017/12/south-korea-govt-to-introduce-tougher.html&quot;, &quot;whitelist_status&quot;: &quot;all_ads&quot;}
</code></pre><p>After generating a file with all the historic posts, I loaded the contents referenced by the &ldquo;url&rdquo; field of each JSON line into <a href="https://github.com/slaveofcode/boilerpipe3">boilerpipe3</a> which can transform full HTML documents into simple text.</p>
<p>
  <img src="/images/reddit/boilerpipe_example.png" alt="An example of how boilerpipe3 works on the reuters article &amp;ldquo;Amazon forecasts jump in holiday sales - and pandemic costs&amp;rdquo;, removing (most of) the unecessary cruft.">
</p>
<p>After loading the article text I did some processing to remove pages that didn&rsquo;t load properly and to truncate articles to ~10k characters at most.</p>
<h3 id="training-the-model">Training the model</h3>
<p>I trained the NLP model for /r/business over a few days on approximately 271k articles from 2018-01-01 to mid-April 2020. I used an <a href="https://docs.fast.ai/text.models.awdlstm">AWD_LSTM</a> (e.g. language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3) and text_classifier_learner(data, AWD_LSTM, drop_mult=0.5)) for the model with classification labels based on the score for neutral (0-10 points), okay (10-100), good (100-500), and great (500+). Note that the classes are kind of arbitrary and I&rsquo;ve changed them around for different iterations and subreddits. The only thing I ended up paying attention to at runtime was the non-neutral class score, so this could have been a binary classifier or even better a classifier that looks at an individual article as a candidate for multiple subreddits rather than training an individual model per subreddit. For world news I had over double the number of articles, that I trained in 300k chunks due to memory limitations.</p>
<h3 id="my-personal-business-news-aggregator">My personal business news aggregator</h3>
<p>I chose a few sites (Business Insider, Reuters, Bloomberg, CNN, CNBC, BBC, etc.) based on some of the frequent sites I noticed for /r/business, and plopped them into a script and built a crawler using requests and BeautifulSoup. Every minute or so I&rsquo;d crawl the site root for new links and process the linked page with boilerpipe and pass it through the NLP model for scoring. New pages with a non-neutral score of greater than 0.25 (an arbitrary threshold I picked for this particular model) would be flagged and emailed to me using AWS SNS. Initially, I relied on filtering these incoming suggestions and submitting the articles myself using the Reddit app.</p>
<p>
  <img src="/images/reddit/example_email.png" alt="An example email I received from my site poller that I called &amp;ldquo;Reddit Postmaster&amp;rdquo;.">
</p>
<h2 id="automation">Automation</h2>
<p>At this point, my site poller is acting as a personal news aggregator that I use to generate suggestions for things to post. To remove me from this loop there are a few &ldquo;last mile&rdquo; problems to solve. Coding up submitting via the Reddit API is easy enough, but I also need to automate gathering an appropriate title for the Reddit post and a final quality-check for the posts getting submitted. The page title is usually not good as a post title (for example containing redundancies like the site name) directly and needs to be reformatted or the title should instead be extracted from the content for example by picking the most appropriate h1 tag on the page. Rather than refining the model to be better at classifying articles and improve the accuracy of the scoring mechanism, and then coding a new component for extracting the title, I decided to try to encapsulate these tasks into a mechanical turk task and have humans as the final gate-keeper and title generator. I can take the results from these mechanical turk tasks and submit the articles to Reddit via the Reddit API utilizing those results.</p>
<h3 id="mechanical-turk">Mechanical Turk</h3>
<p>Mechanical turk is a super neat resource for leveraging humans to complete small tasks for your application. It's great for augmenting AI applications and collecting data via labeling tasks for them. But using it effectively isn't without difficulties. The important thing to know about mechanical turk is that many workers on the platform are (sensibly) optimizing for task completion quantity. When using a custom qualifier for tasks, ensure the qualification you're looking for is not apparent from the question. Similarily a bad HIT (human intelligence task) would be one where you ask the user to read an article and check some box if they think it belongs in some category. For example, I did this with /r/worldnews candidates to ensure that among other things didn't pertain to US news, however the first fully automated submission I made to /r/worldnews was "Johnson & Johnson to stop selling baby powder in the United States" which was removed after receiving 42 upvotes since US internal news is banned on /r/worldnews. Make sure the work you assign at least appears as though you will look at the individual results (e.g. use text input boxes at least once). Be careful and clear about what you're asking and reduce the opportunities for human error. Asking for the user to supply an article title, while well-intentioned results in some users writing poor article titles rather than copying the article title if they can't think of a good one. A better task would be to ask the user to identify and copy the article title as it appears in the article since that has more reliable quality.</p>
<p>
  <img src="/images/reddit/post_notification.png" alt="A new email letting me know when a post was submitted on my behalf.">
</p>
<h2 id="results">Results</h2>
<div class="figure">
    <iframe class="airtable-embed" src="https://airtable.com/embed/shroixznK6muAsosc?backgroundColor=yellow&viewControls=on" frameborder="0" onmousewheel="" width="100%" height="533" style="background: transparent; border: 1px solid #ccc;"></iframe>
    <span class="caption">All items detected and/or posted by "Reddit Postmaster".</span>
</div>
<p>You may notice that we only post a few things per day at most. In fact, our bot is probably under posting. One strategy we could adopt here would be to lower the threshold as time goes on to incentivize posting more content as each post comes with a relatively limited downside (a few downvotes then into obscurity) and a very high upside potential. While we want to avoid being spammy, our behavior is probably a little too conservative.</p>
<h3 id="alignment">Alignment</h3>
<p>The alignment problem (in AI) is when your system performs the way you told it to, for example optimizing upvotes, but it does so in a way that goes against what you actually want. For example regarding the post I referenced earlier related to US news that was posted to /r/worldnews and then removed by a moderator. While the post did get some upvotes (~42) it violated my expectation and preference that the bot would follow the rules. The problem lies in the lack of negative examples of what the bot shouldn&rsquo;t do. Because /r/worldnews isn&rsquo;t full of US news articles that are downvoted and removed for violating the rules, my bot has a lack of examples of what it shouldn&rsquo;t do. We could teach it the rules in the future augmenting the dataset with things that violate the rules with very negative scores. The problem of AI alignment is deeper though, as while I did identify one thing I didn&rsquo;t want the bot to do, there are many unstated things I also don&rsquo;t want the bot to do that it might do to try to optimize upvotes (for example posting photoshopped pictures, offensive content, making jokes about my mother, etc.)</p>
<h3 id="other-means">Other means</h3>
<p>In total over 1.5 months of running the system on two subreddits, I was able to garner around 3.7k Reddit karma. Unfortunately, I never got a big hit on /r/worldnews. I got close on some occasions, such as the article about the grandma being forced to remove photos due to GDPR. However, another article covering the same thing (which later had 20k+ points) was already posted and my post was marked as being covered by another article. For the highly competetive /r/worldnews my mechanical turk tasks were taking just a little too long. This could have likely been remedied by increasing the monetary reward associated with each task. I say &ldquo;around&rdquo; 3.7k because the karma count for each post, in a quantum fashion, actually fluctuates slightly when you attempt to observe it. This is actually to <a href="https://www.reddit.com/r/OutOfTheLoop/comments/21id6d/why_do_number_of_downvotes_on_my_posts_fluctuate/">thwart spam bots</a>. So where did the other 6.3k come from? I got tired of waiting and went for some low hanging fruit I could tackle manually. /r/askreddit was an obvious one, coming up with an engaging question can easily get you 20k+ karma. /r/memes was an entertaining one, but difficult to breakthrough. What ended up being the winner was /r/todayilearned. I dug through &ldquo;interesting facts&rdquo; lists on the internet and simply found things that weren&rsquo;t posted and repackaged the facts.</p>
<p>
  <img src="/images/reddit/the_winning_post.png" alt="Slightly anti-climactic for ~14 years in the making.">
</p>
<p>Interestingly enough, even with such a large number of upvotes on a single post, all the upvotes did not count linearly towards my post karma count and I was still short some, which I was able to grind away on random subreddits (read: terrible memes and dog pictures).</p>
<h3 id="conclusion">Conclusion</h3>
<p>At the end of the day, a bit of manual effort beat out my months of computing time. There is no doubt that I would have automatically crossed the 10k mark and could have easily gone to any arbitrary amount given enough time running my post bot. But still I suppose this reinforces the idea you probably shouldn&rsquo;t go around looking for problems to apply a specific tool to.</p>


</article>


<section class="post-nav">
    <ul>
        
        <li>
            <a href="https://www.a8b.io/posts/principles/"><i class="fa fa-chevron-circle-left"></i> Principles</a>
        </li>
        
        
    </ul>
</section>
  
    
    
  





</main>
    <footer>
        <h6> |
            Rendered by <a href="https://gohugo.io" title="Hugo">Hugo</a> |
            <a href="https://www.a8b.io/index.xml">Subscribe </a></h6>
    </footer>
</div>
<script src="/js/scripts.js"></script>

  
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-126051187-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


</body>

</html>

